{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d053a3ff-fc57-4b13-8d79-0ce63ac7fbce",
   "metadata": {},
   "source": [
    "## Lab 06: The Multi-Layer Perceptron (MLP) and Backpropagation.\n",
    "\n",
    "\n",
    "In this lab we will go from single-neurons to feedforward networks by implementing a simple Multi-Layer Perceptron (MLP) and the famous backpropagation algorithm to train an MLP from labeled data.\n",
    "\n",
    "The MLP extends Perceptrons to multiple layers with one caveat: We are going to switch to continous activation functions instead of the heavyside 0/1 activation first analyzed by Rosenblatt. In our case, we'll use the sigmoid activation function: \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{x}}$$\n",
    "\n",
    "The architecture we will implement is simple, from inp layer to hidden layer to output layer:\n",
    "\n",
    "```3 inputs -> 2 hidden units (sigmoid-activation) -> 1 output unit (sigmoid-activation)```\n",
    "\n",
    "As we go from neurons to networks, all weight vectors of neurons in one layer are collected in a matrix. For example, the equation for the hidden layer is:\n",
    "\n",
    "$$ \\mathbf{h} = \\sigma( W^{(1)} \\mathbf{x} + \\mathbf{b^{(1)}} ) $$\n",
    "\n",
    "where $ W^{(1)} \\in \\mathbb{R}^{m \\times n}$, $ \\mathbf{x} \\in \\mathbb{R}^n$ and $\\mathbf{h},\\mathbf{b} \\in \\mathbb{R}^m $. The original weight vectors for each of the hidden layer perceptrons can be found in the weight matrix as row vectors:\n",
    "\n",
    "$$ W^{(1)} = \\begin{bmatrix}\n",
    "- & \\mathbf{w_1} & - \\\\\n",
    "- & \\mathbf{w_2} & - \\end{bmatrix} $$\n",
    "\n",
    "and $\\mathbf{w_1}, \\mathbf{w_2} \\in \\mathbb{R}^n$. Correspondingly, the output layer is:\n",
    "\n",
    "$$ \\mathbf{h} = \\sigma( W^{(2)} \\mathbf{h} + \\mathbf{b}^{(2)} ) $$\n",
    "\n",
    "Notice, that we are now explicityly tracking biases.\n",
    "\n",
    "To be able to train the network, we will need to be able to quantify its performance using a loss function and minimizing it. We will do so by using the (mean-)squared error (MSE):\n",
    "\n",
    "$$L(\\hat{y},y) = \\frac{1}{2} (\\hat{y} - y)^2$$\n",
    "$$L(\\hat{y},y) = \\frac{1}{2N} \\sum (\\hat{y_i} - y_i)^2$$\n",
    "\n",
    "where $\\hat{y}$ is the prediction, i.e. the output, of our network, and y is the target variable.\n",
    "\n",
    "\n",
    "\n",
    "## Learning objectives\n",
    "1. Practise the mechanics of the forward pass through linear layers + activation\n",
    "1. Translate analytical gradients into numpy code\n",
    "1. See how gradient descent gradually reduces the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822502c7-1b8b-48fe-842b-db5b809d3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "np.random.seed(42)\n",
    "\n",
    "# the sigmoid functions and its derivative\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528f14ac-d0f1-409b-a8a8-0f8a7c0e3ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dummy data\n",
    "n_samples = 200\n",
    "inputs = np.random.uniform(-1, 1, size=(n_samples, 3))\n",
    "\n",
    "true_w = np.array([1.5, -2.0, 0.5])\n",
    "true_b = -0.1\n",
    "targets = sigmoid(inputs @ true_w + true_b)\n",
    "\n",
    "# setup initial network parameters (all weights and biases)\n",
    "n_hidden_units = 2\n",
    "W1 = 0.1 * np.random.randn(n_hidden_units,3)\n",
    "b1 = np.zeros(n_hidden_units)\n",
    "\n",
    "n_output_hidden = 1\n",
    "W2 = 0.1 * np.random.randn(n_output_hidden,2)\n",
    "b2 = np.zeros(n_output_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d3a121-ee8d-4926-a113-86045c49605d",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Your first task is to write a function that implements the forward pass of the network.\n",
    "Use a scatterplot to visualise the random predictions vs the true outputs and calculate the MSE across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38651d3d-42ab-4e6f-ac50-ca4c1492e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the prediction of your network.\n",
    "def forward_pass(inp, W1, b1, W2, b2):\n",
    "    pass\n",
    "\n",
    "# returns the squared loss for one data point\n",
    "def mse_loss(prediction, target):\n",
    "    pass\n",
    "\n",
    "\n",
    "# write a loop over the data that collects all predictions in a list, sums the individual losses. \n",
    "# Then, take the mean of the loss and plot predictions against targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6305bb-e930-4a06-9fc6-455758d86846",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Now train the network! To do so:\n",
    "\n",
    "1. Calculate (analytically) the gradients of weights and biases in your network using the chain rule.\n",
    "2. Implement a forward and backward pass function that calculates the prediction, the loss, and all gradients for the weights and biases using your analytic solution for one data point.\n",
    "3. Train your network for multiple epochs (iterations of the dataset) by updating the parameters with step-size $\\eta$ after every single data-point. (This is the stochastic gradient descent algorithm as immplemented by backpropgataion, or \"batch-size = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0df69c-f0ff-4352-8dbd-0e5933728246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns prediction, loss, dW1, db1, dW2, db2\n",
    "def forward_backward_pass(inp, target, W1, b1, W2, b2):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e479e0a-bf6d-4339-9d98-caa7a0738889",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "eta = 0.5\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    pass\n",
    "\n",
    "# plot the loss over time (as measured in epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8110554d-e725-42dd-8961-bec4cccfc63a",
   "metadata": {},
   "source": [
    "### Task 3 (optional)\n",
    "\n",
    "Try it out on \"real\" data: Look up the famous IRIS dataset for more details, download as per the code.\n",
    "\n",
    "As per good machine learning practice, split the data into training and test and track both training and test error in your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11c7bd32-7b86-4204-aaf8-ab4f362c5900",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "raw = urllib.request.urlopen(url).read().decode(\"utf-8\").strip().split(\"\\n\")\n",
    "\n",
    "rows = [r.split(\",\") for r in raw if r]            # skip empty lines\n",
    "data = np.array(rows)\n",
    "features = data[:, :4].astype(float)\n",
    "\n",
    "inputs = features[:,:3] # sepal length, width, petal length\n",
    "targets = features[:,3] # petal width\n",
    "\n",
    "# reset weights\n",
    "n_hidden_units = 2\n",
    "W1 = 0.1 * np.random.randn(n_hidden_units,3)\n",
    "b1 = np.zeros(n_hidden_units)\n",
    "\n",
    "n_output_hidden = 1\n",
    "W2 = 0.1 * np.random.randn(n_output_hidden,2)\n",
    "b2 = np.zeros(n_output_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cbe3416-f42d-40ad-8cac-e39fe1842d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the training steps from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5658ba97-14dd-4c37-8fa8-fb841f1d6537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.7226293333333333)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d1eb9e-e8fa-4a56-ba09-e028f8edd002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
